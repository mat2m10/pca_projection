{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 09:03:25.812534: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-21 09:03:25.917107: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-21 09:03:26.707079: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-21 09:03:26.710378: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-21 09:03:28.477058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "from functools import reduce\n",
    "\n",
    "# Configuration settings\n",
    "from chasm.config import CONFIG\n",
    "\n",
    "# PLINK preprocessing utilities\n",
    "from chasm.plink_preprocessing import (\n",
    "    concat_AFs, \n",
    "    divide_into_chunks, \n",
    "    make_ids,\n",
    "    make_AFs\n",
    ")\n",
    "\n",
    "from chasm.data_preprocessing import (\n",
    "    is_snp,\n",
    "    make_df,\n",
    "    calculate_AFs,\n",
    "    merge_AFs_ensembl_build,\n",
    "    divide_into_chunks,\n",
    "    align_dataframes,\n",
    ")\n",
    "from chasm.gwas import ols_regression, pca_of_n_snps, project_on_dimensions\n",
    "\n",
    "from chasm.visualization import make_population_plot\n",
    "from chasm.ld_blocks import segmenter\n",
    "from chasm.abyss import linear_abyss\n",
    "\n",
    "from chasm.fst import make_fst, make_global_fst\n",
    "from chasm.cluster import silhouette_score_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsize_chunck = 20_000\\nmin_maf = 0.01\\n\\npath_raw = f\"{CONFIG[\\'PATH_data\\']}/01_raw/geno.pkl\"\\npath_afs = f\"{CONFIG[\\'PATH_data\\']}/02_usefull/allele_frequencies.pkl\"\\npath_output = f\"{CONFIG[\\'PATH_data\\']}/03_macro_similar_AF/\"\\n\\ndivide_into_chunks(path_raw, path_afs, path_output, size_chunck, min_maf)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the GTM data into a pickle file having an id file and calculating the AFs for each SNP \n",
    "# with the CHROM:POS for every corresponding RSID\n",
    "\n",
    "# GTM data\n",
    "\"\"\"\n",
    "path_data_raw = f\"{CONFIG['PATH_data']}/00_raw/recoded_1000G.raw\"\n",
    "path_raw = f\"{CONFIG['PATH_data']}/01_raw/\"\n",
    "path_usefull = f\"{CONFIG['PATH_data']}/02_usefull/\"\n",
    "\n",
    "geno = make_df(path_data_raw, path_usefull, path_raw)\n",
    "\n",
    "afs = calculate_AFs(geno)\n",
    "\n",
    "path_ensembl = f\"/mnt/e/1000G_data/usefull/ensembl_build\"\n",
    "merge_AFs_ensembl_build(path_ensembl, path_usefull, afs)\n",
    "\"\"\"\n",
    "# Divide the AFs into chunks\n",
    "\"\"\"\n",
    "size_chunck = 20_000\n",
    "min_maf = 0.01\n",
    "\n",
    "path_raw = f\"{CONFIG['PATH_data']}/01_raw/geno.pkl\"\n",
    "path_afs = f\"{CONFIG['PATH_data']}/02_usefull/allele_frequencies.pkl\"\n",
    "path_output = f\"{CONFIG['PATH_data']}/03_macro_similar_AF/\"\n",
    "\n",
    "divide_into_chunks(path_raw, path_afs, path_output, size_chunck, min_maf)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose SNPs to project on n dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe raw dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ids = f\"{CONFIG['PATH_data']}/02_usefull/ids.pkl\"\n",
    "path_panel = f\"{CONFIG['PATH_data']}/00_raw/all.panel\"\n",
    "ids = pd.read_pickle(path_ids)\n",
    "labels = pd.read_pickle(path_panel)\n",
    "ids = ids.merge(labels, left_on=\"IID\", right_on=\"Sample name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath_macro_similar = f\"{CONFIG[\\'PATH_data\\']}/03_macro_similar_AF/\"\\niterations = 3\\n\\nfor iter in list(range(iterations)):\\n    print(f\"Running iteration {iter}\")\\n    path_output = f\"{CONFIG[\\'PATH_data\\']}/iteration_{iter}/\"\\n    \\n    if iter == 0:\\n        ids[f\"cluster_{iter}\"] = f\"{0}\"\\n        os.makedirs(f\"{path_output}/pop_{0}\", exist_ok=True)\\n        ids.to_pickle(f\"{path_output}/pop_{0}/ids.pkl\")\\n        \\n    else:\\n        pass\\n    \\n    cluster_to_add = []\\n    for sub_pop in ids[f\"cluster_{iter}\"].unique():\\n        path_output = f\"{CONFIG[\\'PATH_data\\']}/iteration_{iter}/pop_{sub_pop}/\"\\n        os.makedirs(path_output, exist_ok=True)\\n        temp_ids = ids[ids[f\"cluster_{iter}\"] == sub_pop]\\n        nr_of_projected_dimensions = 3\\n        geno = project_on_dimensions(path_macro_similar, path_output, temp_ids, nr_of_projected_dimensions=3, nr_snps = 20_000, n_components = 10)\\n        \\n        existing_dims = []   \\n        for PCs in [f for f in os.listdir(path_output) if f.startswith(\\'PCs\\')]:\\n            dim = PCs.split(\\'PCs_\\')[1].split(\\'.pkl\\')[0]\\n            path_PC = f\"{path_output}/{PCs}\"\\n            PCs = pd.read_pickle(f\"{path_PC}\")\\n            temp_ids = temp_ids.copy()\\n            temp_ids[dim] = list(PCs[\\'PC1\\'])\\n            existing_dims.append(dim)\\n        labels = silhouette_score_clusters(temp_ids, existing_dims, plot=False)\\n        for element in labels:\\n            cluster_to_add.append(f\"{sub_pop}_{element}\")\\n            \\n    ids[f\"cluster_{iter+1}\"] = cluster_to_add\\n    ids.to_pickle(f\"{CONFIG[\\'PATH_data\\']}/iteration_{iter}/ids.pkl\")\\n    \\n    # Vizualization\\n    path_input = f\"{CONFIG[\\'PATH_data\\']}/iteration_{iter}/\"\\n    for pop in [f for f in os.listdir(path_input) if f.startswith(\\'pop\\')]:\\n        pop = pop.split(\\'pop_\\')[1]\\n        temp_ids = ids[ids[f\"cluster_{iter}\"] == pop]\\n        path_pop = f\"{path_input}/pop_{pop}\"\\n        for PCs in [f for f in os.listdir(path_pop) if f.startswith(\\'PCs\\')]:\\n            dim = PCs.split(\\'PCs_\\')[1].split(\\'.pkl\\')[0]\\n            path_PC = f\"{path_pop}/{PCs}\"\\n            PCs = pd.read_pickle(f\"{path_PC}\")\\n            temp_ids = temp_ids.copy()\\n            temp_ids[dim] = list(PCs[\\'PC1\\'])\\n        if len(list(temp_ids[\\'Population name\\'].unique())) > 20:\\n            make_population_plot(temp_ids, \\'dim_1\\', \\'dim_2\\', \\'Superpopulation name\\', f\"iteration {iter} - pop {pop}\", palette = \\'rocket\\')\\n        else:\\n            make_population_plot(temp_ids, \\'dim_1\\', \\'dim_2\\', \\'Population name\\', f\"iteration {iter} - pop {pop}\", palette = \\'rocket\\')\\n            \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path_macro_similar = f\"{CONFIG['PATH_data']}/03_macro_similar_AF/\"\n",
    "iterations = 3\n",
    "\n",
    "for iter in list(range(iterations)):\n",
    "    print(f\"Running iteration {iter}\")\n",
    "    path_output = f\"{CONFIG['PATH_data']}/iteration_{iter}/\"\n",
    "    \n",
    "    if iter == 0:\n",
    "        ids[f\"cluster_{iter}\"] = f\"{0}\"\n",
    "        os.makedirs(f\"{path_output}/pop_{0}\", exist_ok=True)\n",
    "        ids.to_pickle(f\"{path_output}/pop_{0}/ids.pkl\")\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    cluster_to_add = []\n",
    "    for sub_pop in ids[f\"cluster_{iter}\"].unique():\n",
    "        path_output = f\"{CONFIG['PATH_data']}/iteration_{iter}/pop_{sub_pop}/\"\n",
    "        os.makedirs(path_output, exist_ok=True)\n",
    "        temp_ids = ids[ids[f\"cluster_{iter}\"] == sub_pop]\n",
    "        nr_of_projected_dimensions = 3\n",
    "        geno = project_on_dimensions(path_macro_similar, path_output, temp_ids, nr_of_projected_dimensions=3, nr_snps = 20_000, n_components = 10)\n",
    "        \n",
    "        existing_dims = []   \n",
    "        for PCs in [f for f in os.listdir(path_output) if f.startswith('PCs')]:\n",
    "            dim = PCs.split('PCs_')[1].split('.pkl')[0]\n",
    "            path_PC = f\"{path_output}/{PCs}\"\n",
    "            PCs = pd.read_pickle(f\"{path_PC}\")\n",
    "            temp_ids = temp_ids.copy()\n",
    "            temp_ids[dim] = list(PCs['PC1'])\n",
    "            existing_dims.append(dim)\n",
    "        labels = silhouette_score_clusters(temp_ids, existing_dims, plot=False)\n",
    "        for element in labels:\n",
    "            cluster_to_add.append(f\"{sub_pop}_{element}\")\n",
    "            \n",
    "    ids[f\"cluster_{iter+1}\"] = cluster_to_add\n",
    "    ids.to_pickle(f\"{CONFIG['PATH_data']}/iteration_{iter}/ids.pkl\")\n",
    "    \n",
    "    # Vizualization\n",
    "    path_input = f\"{CONFIG['PATH_data']}/iteration_{iter}/\"\n",
    "    for pop in [f for f in os.listdir(path_input) if f.startswith('pop')]:\n",
    "        pop = pop.split('pop_')[1]\n",
    "        temp_ids = ids[ids[f\"cluster_{iter}\"] == pop]\n",
    "        path_pop = f\"{path_input}/pop_{pop}\"\n",
    "        for PCs in [f for f in os.listdir(path_pop) if f.startswith('PCs')]:\n",
    "            dim = PCs.split('PCs_')[1].split('.pkl')[0]\n",
    "            path_PC = f\"{path_pop}/{PCs}\"\n",
    "            PCs = pd.read_pickle(f\"{path_PC}\")\n",
    "            temp_ids = temp_ids.copy()\n",
    "            temp_ids[dim] = list(PCs['PC1'])\n",
    "        if len(list(temp_ids['Population name'].unique())) > 20:\n",
    "            make_population_plot(temp_ids, 'dim_1', 'dim_2', 'Superpopulation name', f\"iteration {iter} - pop {pop}\", palette = 'rocket')\n",
    "        else:\n",
    "            make_population_plot(temp_ids, 'dim_1', 'dim_2', 'Population name', f\"iteration {iter} - pop {pop}\", palette = 'rocket')\n",
    "            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the dims per snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath_data = f\"{CONFIG[\\'PATH_data\\']}/\"\\niteration_files = [f for f in os.listdir(path_data) if f.startswith(\\'iteration\\')]\\nsnp_ids_dfs = []\\nfor iteration in iteration_files:\\n    path_iteration = f\"{path_data}/{iteration}/\"\\n    for pop in [f for f in os.listdir(path_iteration) if f.startswith(\\'pop\\')]:\\n        pop = pop.split(\\'pop_\\')[1]\\n        path_snp_ids = f\"{path_iteration}/pop_{pop}/snp_ids.pkl\"\\n        snp_ids = pd.read_pickle(path_snp_ids)\\n        snp_ids = snp_ids.drop(columns=[\\'pval\\', \\'betas\\', \\'-logp\\'])\\n        snp_ids[f\"{iteration}_pop_{pop}_dim\"] = list(snp_ids[\\'dim\\'])\\n        snp_ids = snp_ids.drop(columns=[\\'dim\\'])\\n        snp_ids = snp_ids.sort_index()  # Now sort\\n        snp_ids_dfs.append(snp_ids)\\n        \\nsnp_ids = reduce(lambda left, right: pd.merge(left, right, on=\\'snp_rs\\'), snp_ids_dfs)\\nsnp_ids.to_pickle(f\"{CONFIG[\\'PATH_data\\']}/02_usefull/post_iterations_snp_ids.pkl\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path_data = f\"{CONFIG['PATH_data']}/\"\n",
    "iteration_files = [f for f in os.listdir(path_data) if f.startswith('iteration')]\n",
    "snp_ids_dfs = []\n",
    "for iteration in iteration_files:\n",
    "    path_iteration = f\"{path_data}/{iteration}/\"\n",
    "    for pop in [f for f in os.listdir(path_iteration) if f.startswith('pop')]:\n",
    "        pop = pop.split('pop_')[1]\n",
    "        path_snp_ids = f\"{path_iteration}/pop_{pop}/snp_ids.pkl\"\n",
    "        snp_ids = pd.read_pickle(path_snp_ids)\n",
    "        snp_ids = snp_ids.drop(columns=['pval', 'betas', '-logp'])\n",
    "        snp_ids[f\"{iteration}_pop_{pop}_dim\"] = list(snp_ids['dim'])\n",
    "        snp_ids = snp_ids.drop(columns=['dim'])\n",
    "        snp_ids = snp_ids.sort_index()  # Now sort\n",
    "        snp_ids_dfs.append(snp_ids)\n",
    "        \n",
    "snp_ids = reduce(lambda left, right: pd.merge(left, right, on='snp_rs'), snp_ids_dfs)\n",
    "snp_ids.to_pickle(f\"{CONFIG['PATH_data']}/02_usefull/post_iterations_snp_ids.pkl\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = f\"{CONFIG['PATH_data']}/\"\n",
    "iterations = [f for f in os.listdir(path_data) if f.startswith('iteration')]\n",
    "for iteration in iterations[0:1]:\n",
    "    path_iteration = f\"{path_data}/{iteration}/\"\n",
    "    for pop in [f for f in os.listdir(path_iteration) if f.startswith('pop')]:\n",
    "        pop = pop.split('pop_')[1]\n",
    "        path_pop = f\"{path_iteration}/pop_{pop}/\"\n",
    "        dims = [f for f in os.listdir(path_pop) if f.startswith('PCs')]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCs_dim_1.pkl', 'PCs_dim_2.pkl', 'PCs_dim_3.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_PCs = 3\n",
    "PCs_labels = []\n",
    "for i in range(nr_PCs):\n",
    "    PCs_labels.append(f\"PC{i+1}\")\n",
    "\n",
    "dims_df = pd.DataFrame()\n",
    "for dim  in dims:\n",
    "    path_dim = f\"{path_pop}/{dim}\"\n",
    "    dim = dim.split('PCs_')[1].split('.pkl')[0]\n",
    "    \n",
    "    PCs = pd.read_pickle(path_dim)\n",
    "    for label in PCs_labels:\n",
    "        PCs[label] = list(PCs['PC1'])\n",
    "        PCs.rename(columns={f\"{label}\": f\"{label}_dim_{dim}\"}, inplace=True)\n",
    "        dims_df[f\"{label}_{dim}\"] = list(PCs[f\"{label}_dim_{dim}\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1_dim_dim_1</th>\n",
       "      <th>PC1_dim_dim_2</th>\n",
       "      <th>PC1_dim_dim_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.667467</td>\n",
       "      <td>3.625981</td>\n",
       "      <td>-0.679535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.898722</td>\n",
       "      <td>2.755374</td>\n",
       "      <td>-8.324177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.563651</td>\n",
       "      <td>6.131908</td>\n",
       "      <td>-4.050449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.625410</td>\n",
       "      <td>2.657043</td>\n",
       "      <td>-1.716168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.471073</td>\n",
       "      <td>5.953194</td>\n",
       "      <td>-1.953593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>-9.241086</td>\n",
       "      <td>-7.202319</td>\n",
       "      <td>-20.577914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>-10.563217</td>\n",
       "      <td>-7.261870</td>\n",
       "      <td>-18.119694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>-9.376725</td>\n",
       "      <td>-6.244336</td>\n",
       "      <td>-18.747823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>-9.779671</td>\n",
       "      <td>-7.542151</td>\n",
       "      <td>-19.379677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>-11.611566</td>\n",
       "      <td>-6.969504</td>\n",
       "      <td>-18.450850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2504 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PC1_dim_dim_1  PC1_dim_dim_2  PC1_dim_dim_3\n",
       "0         27.667467       3.625981      -0.679535\n",
       "1         14.898722       2.755374      -8.324177\n",
       "2         33.563651       6.131908      -4.050449\n",
       "3         27.625410       2.657043      -1.716168\n",
       "4         33.471073       5.953194      -1.953593\n",
       "...             ...            ...            ...\n",
       "2499      -9.241086      -7.202319     -20.577914\n",
       "2500     -10.563217      -7.261870     -18.119694\n",
       "2501      -9.376725      -6.244336     -18.747823\n",
       "2502      -9.779671      -7.542151     -19.379677\n",
       "2503     -11.611566      -6.969504     -18.450850\n",
       "\n",
       "[2504 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca_projection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
