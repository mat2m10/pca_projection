{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "from functools import reduce\n",
    "\n",
    "# Configuration settings\n",
    "from chasm.config import CONFIG\n",
    "\n",
    "# PLINK preprocessing utilities\n",
    "from chasm.plink_preprocessing import (\n",
    "    concat_AFs, \n",
    "    divide_into_chunks, \n",
    "    make_ids,\n",
    "    make_AFs\n",
    ")\n",
    "\n",
    "from chasm.data_preprocessing import (\n",
    "    is_snp,\n",
    "    make_df,\n",
    "    calculate_AFs,\n",
    "    merge_AFs_ensembl_build,\n",
    "    divide_into_chunks,\n",
    "    align_dataframes,\n",
    ")\n",
    "from chasm.gwas import ols_regression, pca_of_n_snps, project_on_dimensions\n",
    "\n",
    "from chasm.visualization import make_population_plot\n",
    "from chasm.ld_blocks import segmenter\n",
    "from chasm.abyss import linear_abyss\n",
    "\n",
    "from chasm.fst import make_fst, make_global_fst\n",
    "from chasm.cluster import silhouette_score_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsize_chunck = 20_000\\nmin_maf = 0.01\\n\\npath_raw = f\"{CONFIG[\\'PATH_data\\']}/01_raw/geno.pkl\"\\npath_afs = f\"{CONFIG[\\'PATH_data\\']}/02_usefull/allele_frequencies.pkl\"\\npath_output = f\"{CONFIG[\\'PATH_data\\']}/03_macro_similar_AF/\"\\n\\ndivide_into_chunks(path_raw, path_afs, path_output, size_chunck, min_maf)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the GTM data into a pickle file having an id file and calculating the AFs for each SNP \n",
    "# with the CHROM:POS for every corresponding RSID\n",
    "\n",
    "# GTM data\n",
    "\"\"\"\n",
    "path_data_raw = f\"{CONFIG['PATH_data']}/00_raw/recoded_1000G.raw\"\n",
    "path_raw = f\"{CONFIG['PATH_data']}/01_raw/\"\n",
    "path_usefull = f\"{CONFIG['PATH_data']}/02_usefull/\"\n",
    "\n",
    "geno = make_df(path_data_raw, path_usefull, path_raw)\n",
    "\n",
    "afs = calculate_AFs(geno)\n",
    "\n",
    "path_ensembl = f\"/mnt/e/1000G_data/usefull/ensembl_build\"\n",
    "merge_AFs_ensembl_build(path_ensembl, path_usefull, afs)\n",
    "\"\"\"\n",
    "# Divide the AFs into chunks\n",
    "\"\"\"\n",
    "size_chunck = 20_000\n",
    "min_maf = 0.01\n",
    "\n",
    "path_raw = f\"{CONFIG['PATH_data']}/01_raw/geno.pkl\"\n",
    "path_afs = f\"{CONFIG['PATH_data']}/02_usefull/allele_frequencies.pkl\"\n",
    "path_output = f\"{CONFIG['PATH_data']}/03_macro_similar_AF/\"\n",
    "\n",
    "divide_into_chunks(path_raw, path_afs, path_output, size_chunck, min_maf)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose SNPs to project on n dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe raw dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ids = f\"{CONFIG['PATH_data']}/02_usefull/ids.pkl\"\n",
    "path_panel = f\"{CONFIG['PATH_data']}/00_raw/all.panel\"\n",
    "ids = pd.read_pickle(path_ids)\n",
    "labels = pd.read_pickle(path_panel)\n",
    "ids = ids.merge(labels, left_on=\"IID\", right_on=\"Sample name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath_macro_similar = f\"{CONFIG[\\'PATH_data\\']}/03_macro_similar_AF/\"\\niterations = 3\\n\\nfor iter in list(range(iterations)):\\n    print(f\"Running iteration {iter}\")\\n    path_output = f\"{CONFIG[\\'PATH_data\\']}/iteration_{iter}/\"\\n    \\n    if iter == 0:\\n        ids[f\"cluster_{iter}\"] = f\"{0}\"\\n        os.makedirs(f\"{path_output}/pop_{0}\", exist_ok=True)\\n        ids.to_pickle(f\"{path_output}/pop_{0}/ids.pkl\")\\n        \\n    else:\\n        pass\\n    \\n    cluster_to_add = []\\n    for sub_pop in ids[f\"cluster_{iter}\"].unique():\\n        path_output = f\"{CONFIG[\\'PATH_data\\']}/iteration_{iter}/pop_{sub_pop}/\"\\n        os.makedirs(path_output, exist_ok=True)\\n        temp_ids = ids[ids[f\"cluster_{iter}\"] == sub_pop]\\n        nr_of_projected_dimensions = 3\\n        geno = project_on_dimensions(path_macro_similar, path_output, temp_ids, nr_of_projected_dimensions=3, nr_snps = 20_000, n_components = 10)\\n        \\n        existing_dims = []   \\n        for PCs in [f for f in os.listdir(path_output) if f.startswith(\\'PCs\\')]:\\n            dim = PCs.split(\\'PCs_\\')[1].split(\\'.pkl\\')[0]\\n            path_PC = f\"{path_output}/{PCs}\"\\n            PCs = pd.read_pickle(f\"{path_PC}\")\\n            temp_ids = temp_ids.copy()\\n            temp_ids[dim] = list(PCs[\\'PC1\\'])\\n            existing_dims.append(dim)\\n        labels = silhouette_score_clusters(temp_ids, existing_dims, plot=False)\\n        for element in labels:\\n            cluster_to_add.append(f\"{sub_pop}_{element}\")\\n            \\n    ids[f\"cluster_{iter+1}\"] = cluster_to_add\\n    ids.to_pickle(f\"{CONFIG[\\'PATH_data\\']}/iteration_{iter}/ids.pkl\")\\n    \\n    # Vizualization\\n    path_input = f\"{CONFIG[\\'PATH_data\\']}/iteration_{iter}/\"\\n    for pop in [f for f in os.listdir(path_input) if f.startswith(\\'pop\\')]:\\n        pop = pop.split(\\'pop_\\')[1]\\n        temp_ids = ids[ids[f\"cluster_{iter}\"] == pop]\\n        path_pop = f\"{path_input}/pop_{pop}\"\\n        for PCs in [f for f in os.listdir(path_pop) if f.startswith(\\'PCs\\')]:\\n            dim = PCs.split(\\'PCs_\\')[1].split(\\'.pkl\\')[0]\\n            path_PC = f\"{path_pop}/{PCs}\"\\n            PCs = pd.read_pickle(f\"{path_PC}\")\\n            temp_ids = temp_ids.copy()\\n            temp_ids[dim] = list(PCs[\\'PC1\\'])\\n        if len(list(temp_ids[\\'Population name\\'].unique())) > 20:\\n            make_population_plot(temp_ids, \\'dim_1\\', \\'dim_2\\', \\'Superpopulation name\\', f\"iteration {iter} - pop {pop}\", palette = \\'rocket\\')\\n        else:\\n            make_population_plot(temp_ids, \\'dim_1\\', \\'dim_2\\', \\'Population name\\', f\"iteration {iter} - pop {pop}\", palette = \\'rocket\\')\\n            \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path_macro_similar = f\"{CONFIG['PATH_data']}/03_macro_similar_AF/\"\n",
    "iterations = 3\n",
    "\n",
    "for iter in list(range(iterations)):\n",
    "    print(f\"Running iteration {iter}\")\n",
    "    path_output = f\"{CONFIG['PATH_data']}/iteration_{iter}/\"\n",
    "    \n",
    "    if iter == 0:\n",
    "        ids[f\"cluster_{iter}\"] = f\"{0}\"\n",
    "        os.makedirs(f\"{path_output}/pop_{0}\", exist_ok=True)\n",
    "        ids.to_pickle(f\"{path_output}/pop_{0}/ids.pkl\")\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    cluster_to_add = []\n",
    "    for sub_pop in ids[f\"cluster_{iter}\"].unique():\n",
    "        path_output = f\"{CONFIG['PATH_data']}/iteration_{iter}/pop_{sub_pop}/\"\n",
    "        os.makedirs(path_output, exist_ok=True)\n",
    "        temp_ids = ids[ids[f\"cluster_{iter}\"] == sub_pop]\n",
    "        nr_of_projected_dimensions = 3\n",
    "        geno = project_on_dimensions(path_macro_similar, path_output, temp_ids, nr_of_projected_dimensions=3, nr_snps = 20_000, n_components = 10)\n",
    "        \n",
    "        existing_dims = []   \n",
    "        for PCs in [f for f in os.listdir(path_output) if f.startswith('PCs')]:\n",
    "            dim = PCs.split('PCs_')[1].split('.pkl')[0]\n",
    "            path_PC = f\"{path_output}/{PCs}\"\n",
    "            PCs = pd.read_pickle(f\"{path_PC}\")\n",
    "            temp_ids = temp_ids.copy()\n",
    "            temp_ids[dim] = list(PCs['PC1'])\n",
    "            existing_dims.append(dim)\n",
    "        labels = silhouette_score_clusters(temp_ids, existing_dims, plot=False)\n",
    "        for element in labels:\n",
    "            cluster_to_add.append(f\"{sub_pop}_{element}\")\n",
    "            \n",
    "    ids[f\"cluster_{iter+1}\"] = cluster_to_add\n",
    "    ids.to_pickle(f\"{CONFIG['PATH_data']}/iteration_{iter}/ids.pkl\")\n",
    "    \n",
    "    # Vizualization\n",
    "    path_input = f\"{CONFIG['PATH_data']}/iteration_{iter}/\"\n",
    "    for pop in [f for f in os.listdir(path_input) if f.startswith('pop')]:\n",
    "        pop = pop.split('pop_')[1]\n",
    "        temp_ids = ids[ids[f\"cluster_{iter}\"] == pop]\n",
    "        path_pop = f\"{path_input}/pop_{pop}\"\n",
    "        for PCs in [f for f in os.listdir(path_pop) if f.startswith('PCs')]:\n",
    "            dim = PCs.split('PCs_')[1].split('.pkl')[0]\n",
    "            path_PC = f\"{path_pop}/{PCs}\"\n",
    "            PCs = pd.read_pickle(f\"{path_PC}\")\n",
    "            temp_ids = temp_ids.copy()\n",
    "            temp_ids[dim] = list(PCs['PC1'])\n",
    "        if len(list(temp_ids['Population name'].unique())) > 20:\n",
    "            make_population_plot(temp_ids, 'dim_1', 'dim_2', 'Superpopulation name', f\"iteration {iter} - pop {pop}\", palette = 'rocket')\n",
    "        else:\n",
    "            make_population_plot(temp_ids, 'dim_1', 'dim_2', 'Population name', f\"iteration {iter} - pop {pop}\", palette = 'rocket')\n",
    "            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the dims per snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath_data = f\"{CONFIG[\\'PATH_data\\']}/\"\\niteration_files = [f for f in os.listdir(path_data) if f.startswith(\\'iteration\\')]\\nsnp_ids_dfs = []\\nfor iteration in iteration_files:\\n    path_iteration = f\"{path_data}/{iteration}/\"\\n    for pop in [f for f in os.listdir(path_iteration) if f.startswith(\\'pop\\')]:\\n        pop = pop.split(\\'pop_\\')[1]\\n        path_snp_ids = f\"{path_iteration}/pop_{pop}/snp_ids.pkl\"\\n        snp_ids = pd.read_pickle(path_snp_ids)\\n        snp_ids = snp_ids.drop(columns=[\\'pval\\', \\'betas\\', \\'-logp\\'])\\n        snp_ids[f\"{iteration}_pop_{pop}_dim\"] = list(snp_ids[\\'dim\\'])\\n        snp_ids = snp_ids.drop(columns=[\\'dim\\'])\\n        snp_ids = snp_ids.sort_index()  # Now sort\\n        snp_ids_dfs.append(snp_ids)\\n        \\nsnp_ids = reduce(lambda left, right: pd.merge(left, right, on=\\'snp_rs\\'), snp_ids_dfs)\\nsnp_ids.to_pickle(f\"{CONFIG[\\'PATH_data\\']}/02_usefull/post_iterations_snp_ids.pkl\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path_data = f\"{CONFIG['PATH_data']}/\"\n",
    "iteration_files = [f for f in os.listdir(path_data) if f.startswith('iteration')]\n",
    "snp_ids_dfs = []\n",
    "for iteration in iteration_files:\n",
    "    path_iteration = f\"{path_data}/{iteration}/\"\n",
    "    for pop in [f for f in os.listdir(path_iteration) if f.startswith('pop')]:\n",
    "        pop = pop.split('pop_')[1]\n",
    "        path_snp_ids = f\"{path_iteration}/pop_{pop}/snp_ids.pkl\"\n",
    "        snp_ids = pd.read_pickle(path_snp_ids)\n",
    "        snp_ids = snp_ids.drop(columns=['pval', 'betas', '-logp'])\n",
    "        snp_ids[f\"{iteration}_pop_{pop}_dim\"] = list(snp_ids['dim'])\n",
    "        snp_ids = snp_ids.drop(columns=['dim'])\n",
    "        snp_ids = snp_ids.sort_index()  # Now sort\n",
    "        snp_ids_dfs.append(snp_ids)\n",
    "        \n",
    "snp_ids = reduce(lambda left, right: pd.merge(left, right, on='snp_rs'), snp_ids_dfs)\n",
    "snp_ids.to_pickle(f\"{CONFIG['PATH_data']}/02_usefull/post_iterations_snp_ids.pkl\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca_projection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
